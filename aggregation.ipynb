{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import linecache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating postcodes by alphabetical prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id_og = linecache.getline(r\"CaseStudyData.txt\", 17).split(\" \")[2:-1]\n",
    "customer_id_og = [id.strip('\"') for id in customer_id_og]\n",
    "\n",
    "candidate_id_og = linecache.getline(r\"CaseStudyData.txt\", 27).split(\" \")[2:-1]\n",
    "candidate_id_og = [id.strip('\"') for id in candidate_id_og]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_postcodes(list):\n",
    "    \n",
    "    # Extract unique prefixes\n",
    "    unique_prefixes = np.unique([postcode[:2] if len(postcode) >= 2 and postcode[1].isalpha() else postcode[0] for postcode in list])\n",
    "\n",
    "    # Create array with prefixes\n",
    "    aggregated_values = np.array([postcode[:2] if len(postcode) >= 2 and postcode[1].isalpha() else postcode[0] for postcode in list])\n",
    "\n",
    "    return unique_prefixes, aggregated_values\n",
    "\n",
    "\n",
    "# Display the aggregated values\n",
    "# for i, prefix in enumerate(unique_prefixes):\n",
    "#     count = np.count_nonzero(aggregated_values == prefix)\n",
    "#     print(f\"Aggregate for prefix {prefix}: {count}\")\n",
    "\n",
    "customer_id = aggregate_postcodes(customer_id_og)\n",
    "candidate_id = aggregate_postcodes(candidate_id_og)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I think the CustomerID and CandidateID vectors are all the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the 1D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aggregated_values(postcode_list, value_list, operation='average'):\n",
    "    \n",
    "    '''\n",
    "    postcode_list: the reference data to obtain the unique postcode prefixes from\n",
    "    value_list: the vector we wish to aggregate/group\n",
    "    operation: can choose between averging, summing, or taking the maximum value when aggregating\n",
    "    \n",
    "    This function takes the arguments listed above and returns a vector of \n",
    "    aggregated values based on the operation chosen.\n",
    "    '''\n",
    "    \n",
    "    unique_prefixes, aggregated_values = aggregate_postcodes(postcode_list)\n",
    "    \n",
    "    values_dict = {prefix: [] for prefix in unique_prefixes}\n",
    "\n",
    "    for prefix, value in zip(aggregated_values, value_list):\n",
    "        values_dict[prefix].append(value)\n",
    "\n",
    "    if operation == 'average':\n",
    "        result_array = np.array([np.mean(values_dict[prefix]) for prefix in unique_prefixes])\n",
    "    elif operation == 'sum':\n",
    "        result_array = np.array([np.sum(values_dict[prefix]) for prefix in unique_prefixes])\n",
    "    elif operation == 'maximum':\n",
    "        result_array = np.array([np.max(values_dict[prefix]) for prefix in unique_prefixes])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation. Please choose 'average', 'sum', or 'maximum'.\")\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_easting = linecache.getline(r\"CaseStudyData.txt\", 20).split(\" \")[2:-1]\n",
    "customer_easting = [eval(coord) for coord in customer_easting]\n",
    "customer_easting_avg = process_aggregated_values(customer_id_og, customer_easting)\n",
    "\n",
    "customer_northing = linecache.getline(r\"CaseStudyData.txt\", 21).split(\" \")[2:-1]\n",
    "customer_northing = [eval(coord) for coord in customer_northing]\n",
    "customer_northing_avg = process_aggregated_values(customer_id_og, customer_northing)\n",
    "\n",
    "candidate_easting = linecache.getline(r\"CaseStudyData.txt\", 30).split(\" \")[2:-1]\n",
    "candidate_easting = [eval(coord) for coord in candidate_easting]\n",
    "candidate_easting_avg = process_aggregated_values(candidate_id_og, candidate_easting)\n",
    "\n",
    "candidate_northing = linecache.getline(r\"CaseStudyData.txt\", 31).split(\" \")[2:-1]\n",
    "candidate_northing = [eval(coord) for coord in candidate_northing]\n",
    "candidate_northing_avg = process_aggregated_values(candidate_id_og, candidate_northing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating  warehouse costs and capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_costs_capacities(line_start, line_stop):\n",
    "    \n",
    "    '''\n",
    "    Simply takes the line to start reading, and the line to stop reading. Then, it \n",
    "    processes and transforms the 1D cost and capacity vectors into a format that we can use.\n",
    "    '''\n",
    "    \n",
    "    file = open(r\"CaseStudyData.txt\", \"r\")\n",
    "    costs_og = file.readlines()[line_start-1:line_stop]\n",
    "    costs_og = list(map(lambda s: s.strip(), costs_og))\n",
    "\n",
    "    costs_og = [costs_og[line].split(\" \") for line in range(len(costs_og))]\n",
    "    costs = []\n",
    "    for line in costs_og:\n",
    "        for i in range(len(line)):\n",
    "            costs.append(line[i].strip('[]'))\n",
    "            \n",
    "    costs = costs[2:]        \n",
    "    costs = [eval(cost) for cost in costs]\n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_costs = process_costs_capacities(62, 105)\n",
    "operating_costs = process_costs_capacities(108, 146)\n",
    "wh_capacities = process_costs_capacities(149, 193)\n",
    "\n",
    "setup_costs_agg = process_aggregated_values(customer_id_og, setup_costs, operation = \"maximum\")\n",
    "operating_costs_agg = process_aggregated_values(customer_id_og, operating_costs, operation = \"maximum\")\n",
    "wh_capacities_agg = process_aggregated_values(customer_id_og, wh_capacities, operation = \"maximum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the 2D arrays\n",
    "\n",
    "We start off with 4 $\\times$ 440 = 1760 values, so we expect to be left with 4 vectors in the end - one for each product type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nD(line_start, line_stop, no_to_delete = None):\n",
    "    \n",
    "    '''\n",
    "    Very similar to the function for processing costs and capacities, but this \n",
    "    one doesn't automatically remove the first two elements from the beginning like\n",
    "    the other one. This is because the formatting for 2D stuff is a little different \n",
    "    and requires some further processing outside the function too.\n",
    "    \n",
    "    The logic is to basically get it all cleaned up within the function, then \n",
    "    reshape and delete columns outside.\n",
    "    '''\n",
    "    \n",
    "    file = open(r\"CaseStudyData.txt\", \"r\")\n",
    "    list_og = file.readlines()[line_start-1:line_stop]\n",
    "    list_og = list(map(lambda s: s.strip(), list_og))\n",
    "\n",
    "    list_og = [list_og[line].split(\" \") for line in range(len(list_og))]\n",
    "    processed_list = []\n",
    "    for line in list_og:\n",
    "        for i in range(len(line)):\n",
    "            processed_list.append(line[i].strip('[]'))\n",
    "    \n",
    "    if no_to_delete != None:\n",
    "        processed_list = processed_list[no_to_delete:]\n",
    "    else:\n",
    "        pass\n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_per_product = np.asarray(process_nD(197, 343)).reshape(440, 6)[:, 2:]\n",
    "demand_per_product = demand_per_product.astype(int)\n",
    "\n",
    "dpp_agg = []\n",
    "for i in range(4):\n",
    "    dpp_agg.append(process_aggregated_values(customer_id_og, demand_per_product[:,i], \"average\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 15)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpp_per_year = np.asarray(process_nD(347, 1693, 1)).reshape(440, 4, -1)[:, :, 3:]\n",
    "dpp_per_year = dpp_per_year.astype(int)\n",
    "\n",
    "dpp_py_agg = []\n",
    "for i in range(4):\n",
    "    dpp_aggregate = []\n",
    "    for j in range(10):\n",
    "        dpp_aggregate.append(process_aggregated_values(customer_id_og, dpp_per_year[:, i, j], \"average\"))\n",
    "    dpp_py_agg.append(dpp_aggregate)\n",
    "\n",
    "np.asarray(dpp_py_agg).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the 4D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 100, 15)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dppy_scenarios = np.asarray(process_nD(1695, 115666, 1)).reshape(440, 4, 10, -1)[:, :, :, 4:]\n",
    "dppy_scenarios  = dppy_scenarios.astype(int)\n",
    "\n",
    "dppys_agg = []\n",
    "for i in range(4):\n",
    "    dppy_agg = []\n",
    "    for j in range(10):\n",
    "        dp_agg = []\n",
    "        for k in range(100):\n",
    "            dp_agg.append(process_aggregated_values(customer_id_og, dppy_scenarios[:, i, j, k], \"average\"))\n",
    "        dppy_agg.append(dp_agg)\n",
    "    dppys_agg.append(dppy_agg)\n",
    "    \n",
    "np.asarray(dppys_agg).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
